---
title: "The Car Project - Netzer Validation Method"
author: "Ankit Sisodia"
date: "Jan 15, 2024"
output: html_document
---

## Importing Libraries

```{r}

library(caret)
library(cowplot)
library(doParallel) # For parallel processing
library(dplyr)
library(data.table)
library(fixest)
library(foreign)
library(geometry)
library(ggplot2)
library(ggalt)
library(ggrepel)
library(grid)
library(jpeg)
library(lpdensity)
library(lubridate)
library(magick)
library(MASS)  # for the isoMDS function
library(purrr)
library(ranger) # Faster Random Forest implementation
library(randomForest)
library(rddensity)
library(rdrobust)
library(rdlocrand)
library(readstata13)
library(readxl)
library(reshape)
library(sp)
library(sf)
library(SnowballC)
library(stargazer)
library(stringr)
require(tidyr)
library(tm)
library(xgboost)
library(xtable)

rm(list=ls())

```

## Reading Files

```{r}

df5_ext <- read_excel("./honestjohn_validation/extracted_till_5.xlsx") %>% distinct()
df5_ext$Customer <- NULL
df6_ext <- read_excel("./honestjohn_validation/extracted6.xlsx") %>% distinct()
df7a_ext <- read_excel("./honestjohn_validation/extracted7a.xlsx") %>% distinct()
df7b_ext <- read_excel("./honestjohn_validation/extracted7b.xlsx") %>% distinct()
df7c_ext <- read_excel("./honestjohn_validation/extracted7c.xlsx") %>% distinct()
df7d_ext <- read_excel("./honestjohn_validation/extracted7d.xlsx") %>% distinct()
df7e_ext <- read_excel("./honestjohn_validation/extracted7e.xlsx") %>% distinct()
df7f_ext <- read_excel("./honestjohn_validation/extracted7f.xlsx") %>% distinct()
df8a_ext <- read_excel("./honestjohn_validation/extracted8a.xlsx") %>% distinct()
df8b_ext <- read_excel("./honestjohn_validation/extracted8b.xlsx") %>% distinct()
df9a_ext <- read_excel("./honestjohn_validation/extracted9a.xlsx") %>% distinct()
df9b_ext <- read_excel("./honestjohn_validation/extracted9b.xlsx") %>% distinct()
df9c_ext <- read_excel("./honestjohn_validation/extracted9c.xlsx") %>% distinct()
df9d_ext <- read_excel("./honestjohn_validation/extracted9d.xlsx") %>% distinct()
df9e_ext <- read_excel("./honestjohn_validation/extracted9e.xlsx") %>% distinct()
df9f_ext <- read_excel("./honestjohn_validation/extracted9f.xlsx") %>% distinct()
# 
df <- rbind(df5_ext,df6_ext,df7a_ext,df7b_ext,df7c_ext,df7d_ext,df7e_ext,df7f_ext,df8a_ext,df8b_ext,df9a_ext,df9b_ext,df9c_ext,df9d_ext,df9e_ext,df9f_ext) %>% distinct()

df$orig_date <- df$Date
df$Day <- substr(df$Date,1,3)
df$Time <- substr(df$Date,nchar(df$Date)-4,nchar(df$Date))
df$Date <- substr(df$Date,5,nchar(df$Date)-6)
df$Year <- substr(df$Date,nchar(df$Date)-3,nchar(df$Date))
df$Month <- substr(df$Date,nchar(df$Date)-7,nchar(df$Date)-5)
df$Month <- ifelse(df$Month=="Jan","01",ifelse(df$Month=="Feb","02",ifelse(df$Month=="Mar","03",ifelse(df$Month=="Apr","04",ifelse(df$Month=="May","05",ifelse(df$Month=="Jun","06",ifelse(df$Month=="Jul","07",ifelse(df$Month=="Aug","08",ifelse(df$Month=="Sep","09",ifelse(df$Month=="Oct","10",ifelse(df$Month=="Nov","11",ifelse(df$Month=="Dec","12",NA))))))))))))
df$Date <- substr(df$Date,1,nchar(df$Date)-9)
df$Date <- ifelse(df$Date=="1","01",ifelse(df$Date=="2","02",ifelse(df$Date=="3","03",ifelse(df$Date=="4","04",ifelse(df$Date=="5","05",ifelse(df$Date=="6","06",ifelse(df$Date=="7","07",ifelse(df$Date=="8","08",ifelse(df$Date=="9","09",df$Date)))))))))
df$converted_dates <- paste0(df$Year,"-",df$Month,"-",df$Date)

table(df$Date)
table(df$Month)
table(df$Year)

range(as.Date(df$converted_dates))

rm(df5_ext,df6_ext,df7a_ext,df7b_ext,df7c_ext,df7d_ext,df7e_ext,df7f_ext,df8a_ext,df8b_ext,df9a_ext,df9b_ext,df9c_ext,df9d_ext,df9e_ext,df9f_ext)

old_df <- df

```

## Reading Files

```{r}

df1 <- read.csv("./honestjohn_validation/extracted2016.csv") %>% distinct()
df2 <- read.csv("./honestjohn_validation/extracted2006-7.csv") %>% distinct()
df3 <- read.csv("./honestjohn_validation/extracted2007-8.csv") %>% distinct()
df4 <- read.csv("./honestjohn_validation/extracted2008-10.csv") %>% distinct()
df5 <- read.csv("./honestjohn_validation/extracted2011a.csv") %>% distinct()
df6 <- read.csv("./honestjohn_validation/extracted2011b.csv") %>% distinct()
df7 <- read.csv("./honestjohn_validation/extracted2012.csv") %>% distinct()
df8 <- read.csv("./honestjohn_validation/extracted2013.csv") %>% distinct()
df9 <- read.csv("./honestjohn_validation/extracted2014.csv") %>% distinct()
df10 <- read.csv("./honestjohn_validation/extracted2015.csv") %>% distinct()
df11 <- read.csv("./honestjohn_validation/extracted2017.csv") %>% distinct()
df12 <- read.csv("./honestjohn_validation/extracted2018.csv") %>% distinct()
df13 <- read.csv("./honestjohn_validation/extracted2019.csv") %>% distinct()

df <- rbind(df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13) %>% distinct()

df$orig_date <- df$Date
df$Day <- substr(df$Date,1,3)
df$Time <- substr(df$Date,nchar(df$Date)-4,nchar(df$Date))
df$Date <- substr(df$Date,5,nchar(df$Date)-6)
df$Year <- substr(df$Date,nchar(df$Date)-3,nchar(df$Date))
df$Month <- substr(df$Date,nchar(df$Date)-7,nchar(df$Date)-5)
df$Month <- ifelse(df$Month=="Jan","01",ifelse(df$Month=="Feb","02",ifelse(df$Month=="Mar","03",ifelse(df$Month=="Apr","04",ifelse(df$Month=="May","05",ifelse(df$Month=="Jun","06",ifelse(df$Month=="Jul","07",ifelse(df$Month=="Aug","08",ifelse(df$Month=="Sep","09",ifelse(df$Month=="Oct","10",ifelse(df$Month=="Nov","11",ifelse(df$Month=="Dec","12",NA))))))))))))
df$Date <- substr(df$Date,1,nchar(df$Date)-9)
df$Date <- ifelse(df$Date=="1","01",ifelse(df$Date=="2","02",ifelse(df$Date=="3","03",ifelse(df$Date=="4","04",ifelse(df$Date=="5","05",ifelse(df$Date=="6","06",ifelse(df$Date=="7","07",ifelse(df$Date=="8","08",ifelse(df$Date=="9","09",df$Date)))))))))
df$converted_dates <- paste0(df$Year,"-",df$Month,"-",df$Date)

table(df$Date)
table(df$Month)
table(df$Year)

range(as.Date(df$converted_dates))

rm(df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13)

df14 <- read.csv("./honestjohn_validation/extracted2010.csv") %>% distinct()

df14$orig_date <- df14$Date
df14$Day <- NA
df14$Time <- substr(df14$orig_date,12,19)
df14$Date <- substr(df14$orig_date,9,10)
df14$Year <- substr(df14$orig_date,1,4)
df14$Month <- substr(df14$orig_date,6,7)
df14$converted_dates <- paste0(df14$Year,"-",df14$Month,"-",df14$Date)

df <- rbind(df,df14) %>% distinct()
rm(df14)

table(df$Date)
table(df$Month)
table(df$Year)

new_df <- df
rm(df)

```

## Combining older approach and newer approach

```{r}

df <- rbind(old_df,new_df)
print(length(unique(df$Title)))
count_sentences <- function(text) {
  # Split the text by sentence-ending punctuation (., !, ?)
  sentences <- str_split(text, "(?<=[.!?])\\s+", simplify = TRUE)
  # Count the number of sentences
  return(length(sentences))
}
df$SentenceCount <- sapply(df$Message, count_sentences)
print(sum(df$SentenceCount))
df <- unique(df[,c("orig_date","converted_dates","Message","Year")])
# df <- df %>% distinct()

stop_words <- stopwords("en")
# df$title_message <- paste(df$Title,df$Message)
df$title_message <- removeWords(tolower(df$Message), stop_words)
df$title_message <- wordStem(df$title_message, language = "en")
df$title_message <- gsub("[^[:alnum:] ]", "", df$title_message)
df$title_message <- gsub("[[:punct:]]", "", df$title_message)
# df$title_message <- gsub("  ", " ", df$title_message)
df$Message <- NULL
df <- df %>% distinct()

table(df$Year)

# rm(old_df,new_df)

```

## BLP Data

```{r}

uk_product_data <- read.csv('exp_uk_product_data.csv', stringsAsFactors=FALSE)

uk_product_data$old_segment_desc <- uk_product_data$Segment_Desc
uk_product_data$segment_name <- ifelse(uk_product_data$Segment_Desc=="A-Segment (Minicars)","A",ifelse(uk_product_data$Segment_Desc=="B-segment (subcompact)","B",ifelse(uk_product_data$Segment_Desc=="C-segment (compact)","C",ifelse(uk_product_data$Segment_Desc=="D-segment (mid-size)","D",ifelse(uk_product_data$Segment_Desc=="E-segment (mid-size luxury sedan)","E",ifelse(uk_product_data$Segment_Desc=="J-segment (SUV)","J",ifelse(uk_product_data$Segment_Desc=="M-segment (MPV)","M","X")))))))
uk_product_data$Segment_Desc <- NULL

uk_product_data <- uk_product_data %>% dplyr::select(clustering_ids,make,model,market_ids,segment_name,car_ids,firm_ids,quantity,shares)

select_image_table <- read.csv('exp_selected_python_image_table.csv',stringsAsFactors = FALSE)

df_data <- merge(uk_product_data,select_image_table,all.x = TRUE)
df_data$Image_name <- NULL

top_makes_for_latex <- df_data %>% filter(segment_name == "B" | segment_name == "D" | segment_name == "J") %>% group_by(make) %>% summarise(total_shares = sum(shares)) %>% slice_max(order_by = total_shares, n = 5) %>% pull(make)
segment_value <- sort(unique(df_data$segment_name))

rm(select_image_table,uk_product_data)

```

## Pre-processing

```{r}

# Function to generate a regex for basic spelling tolerance (e.g., missing one letter)
function_regex <- function(text_string) {
  # Replace spaces in the make/model name with a regex that allows spaces, hyphens, or periods
  # Convert make/model to lowercase for case insensitivity in matching
  escaped_text_string <- tolower(text_string)
  escaped_text_string <- gsub(" ", "[-\\. ]*", escaped_text_string) # Allow zero or more of -, ., or space between words
  escaped_text_string <- gsub("([\\.\\+\\*\\?\\[\\^\\]\\$\\(\\)\\{\\}\\=\\!\\<\\>\\|\\:\\-])", "\\\\\\1", escaped_text_string) # Escape special regex characters

  # Create a regex pattern that ensures the match is bounded by spaces or start/end of the string
  regex <- paste0("(^|\\s)", escaped_text_string, "($|\\s)")
  return(regex)
}

makemodel_mds <- function(market) {
  df_filtered <- df %>% filter(Year==market)
  df_temp <- df_data %>% filter(market_ids==market) %>% dplyr::select(make)
  unique_makemodel <- unique(df_data[,c("clustering_ids","make","model","segment_name")]) %>% filter(model!="500" & model!="GS")

  for(makemodel in unique_makemodel$model) {
  regex <- function_regex(makemodel)  # Generate the regex for each model
  variable_name <- paste0("is_makemodel_", tolower(gsub("[-\\. ]", "_", makemodel)))  # Create a valid variable name
  df_filtered[[variable_name]] <- grepl(regex, df_filtered$title_message, perl = TRUE)  # Use perl = TRUE for complex regex
  }
  
  df_makemodel <- df_filtered[,c(5:ncol(df_filtered))]
  
  prob_makemodel <- colMeans(df_makemodel)
  makemodel <- names(prob_makemodel)  # get column names
  prob_makemodel_joint <- matrix(nrow = ncol(df_makemodel), ncol = ncol(df_makemodel), dimnames = list(makemodel, makemodel))
  
  for (i in 1:ncol(df_makemodel)) {
    for (j in 1:ncol(df_makemodel)) {
      if (i != j) {
      # Probability of both makes appearing in the same message
      prob_makemodel_joint[i, j] <- mean(df_makemodel[, makemodel[i]] & df_makemodel[, makemodel[j]])
    } else {
      # The diagonal should be the probability of the make itself
      prob_makemodel_joint[i, j] <- prob_makemodel[i]
    }
  }
}
  lift_matrix <- matrix(nrow = ncol(df_makemodel), ncol = ncol(df_makemodel), dimnames = list(unique_makemodel$clustering_ids, unique_makemodel$clustering_ids))

  for (i in 1:ncol(df_makemodel)) {
    for (j in 1:ncol(df_makemodel)) {
      lift_matrix[i, j] <- ifelse(prob_makemodel[i] * prob_makemodel[j] == 0, 0, prob_makemodel_joint[i, j] / (prob_makemodel[i] * prob_makemodel[j]))
    }
    }
  
  diag(lift_matrix) <- 0
  makemodel_lift_matrix <- lift_matrix
  
  dist_matrix <- 1 / makemodel_lift_matrix
  dist_matrix[is.infinite(dist_matrix)] <- max(dist_matrix[is.finite(dist_matrix)]) * 10 # Set infinite values to a large number
  dist_matrix <- as.dist(dist_matrix)
  mds_result <- cmdscale(dist_matrix)
  
  mds_coords <- data.frame(f_x = mds_result[, 1], f_y = mds_result[, 2], clustering_ids = unique_makemodel$clustering_ids, make = unique_makemodel$make, model = unique_makemodel$model, segment_name = unique_makemodel$segment_name)
  
  return(mds_coords)
  
}
```

## Plot Function

```{r}

main_mds_function <- function(mds_data_df, segment_text1, segment_text2, segment_text3, market) {

df_temp <- mds_data_df
df_temp <- df_temp %>% filter(segment_name==segment_text1 | segment_name==segment_text2 | segment_name==segment_text3)
df_temp <- df_temp %>% filter(make %in% top_makes_for_latex)
df <- df_temp
char1 <- "f_x"
char2 <- "f_y"

p <- ggplot(df, aes_string(x=char1, y=char2)) + geom_point(aes(shape=make, color=segment_name), alpha=0.95, size=4) + theme_minimal() + guides(size=FALSE) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.line = element_line(colour = "black"), legend.text = element_text(size=16), axis.title.x = element_text(size=16), axis.title.y = element_text(size=16), axis.text.x = element_text(size=16), axis.text.y = element_text(size=16)) + scale_x_continuous(limits = c(1.2*min(df_temp[[char1]]), 1.2*max(df_temp[[char1]]))) + scale_y_continuous(limits = c(1.2*min(df_temp[[char2]]), 1.2*max(df_temp[[char2]]))) + scale_color_discrete(guide = guide_legend(title = NULL)) + scale_shape_manual(values=1:(nlevels(factor(df$make))),guide = guide_legend(title = NULL)) + theme(legend.position = "bottom", legend.box = "vertical") + xlab("X-AXIS") + ylab("Y-AXIS")  + theme(plot.title = element_text(hjust = 0.5)) + geom_text_repel(aes(label=model), hjust=0, vjust=0, size=6, alpha = 0.9)

for (seg in segment_value) {
  df_seg = df[df$segment_name == seg, ]
  p <- p + geom_encircle(data = df_seg, aes_string(x=char1, y=char2, color = "segment_name"), alpha = 0.3, expand = 0.01)
}

ggsave(paste0("./hj_mds_plot_files/model_plotD_mds_",segment_text1,segment_text2,segment_text3,"-",market,"_",char1,"_",char2,".pdf"), plot=p, dpi = 300, height = 9, width = 9, units = "in")

}

```

## Execution

```{r}

for(market in 2008:2017){
  mds_result <- makemodel_mds(market)
  write.csv(mds_result,paste0("./input_validation_test/mds_forum_", market,".csv"),row.names = FALSE)
  main_mds_function(mds_result,"B","D","J",market)
}

```

## Copying Files from Input to Output

```{r}

for(year in 2008:2017){
    structured_data <- read.csv(paste0("./input_validation_test/mds_structured_",year,".csv"),stringsAsFactors = FALSE)
    visual_data <- read.csv(paste0("./input_validation_test/mds_visual_",year,".csv"),stringsAsFactors = FALSE)
    combination_data <- read.csv(paste0("./input_validation_test/mds_combination_",year,".csv"),stringsAsFactors = FALSE)
    forum_data <- read.csv(paste0("./input_validation_test/mds_forum_",year,".csv"),stringsAsFactors = FALSE)
    
    write.csv(structured_data,paste0("./output_validation_test/mds_structured_", year,".csv"),row.names = FALSE)
    write.csv(visual_data,paste0("./output_validation_test/mds_visual_", year,".csv"),row.names = FALSE)
    write.csv(combination_data,paste0("./output_validation_test/mds_combination_", year,".csv"),row.names = FALSE)
    write.csv(forum_data,paste0("./output_validation_test/mds_forum_", year,".csv"),row.names = FALSE)
}

```

## Cleaning Up Forum Data

```{r}

for(year in 2008:2017){
    forum_data <- read.csv(paste0("./output_validation_test/mds_forum_",year,".csv"),stringsAsFactors = FALSE)
    print(dim(forum_data))
    structured_data <- read.csv(paste0("./output_validation_test/mds_structured_",year,".csv"),stringsAsFactors = FALSE)
    forum_data <- merge(forum_data,structured_data[ , "clustering_ids", drop = FALSE],by = "clustering_ids")
    print(dim(forum_data))
    write.csv(forum_data,paste0("./output_validation_test/mds_forum_", year,".csv"),row.names = FALSE)
}

for(year in 2008:2017){
    structured_data <- read.csv(paste0("./output_validation_test/mds_structured_",year,".csv"),stringsAsFactors = FALSE)
    visual_data <- read.csv(paste0("./output_validation_test/mds_visual_",year,".csv"),stringsAsFactors = FALSE)
    combination_data <- read.csv(paste0("./output_validation_test/mds_combination_",year,".csv"),stringsAsFactors = FALSE)
    forum_data <- read.csv(paste0("./output_validation_test/mds_forum_",year,".csv"),stringsAsFactors = FALSE)
    structured_data <- merge(structured_data,forum_data[ , "clustering_ids", drop = FALSE],by = "clustering_ids")
    visual_data <- merge(visual_data,forum_data[ , "clustering_ids", drop = FALSE],by = "clustering_ids")
    combination_data <- merge(combination_data,forum_data[ , "clustering_ids", drop = FALSE],by = "clustering_ids")
    
    print(dim(forum_data))
    print(dim(structured_data))
    print(dim(visual_data))
    print(dim(combination_data))

    structured_data <- structured_data[,c("clustering_ids","s_x","s_y")]
    visual_data <- visual_data[,c("clustering_ids","v_x","v_y")]
    combination_data <- combination_data[,c("clustering_ids","m_x","m_y")]
    forum_data <- forum_data[,c("clustering_ids","f_x","f_y")]
    
    names(structured_data) <- c("clustering_ids","x","y")
    names(visual_data) <- c("clustering_ids","x","y")
    names(combination_data) <- c("clustering_ids","x","y")
    names(forum_data) <- c("clustering_ids","x","y")
    
    write.csv(structured_data,paste0("./output_validation_test/mds_structured_", year,".csv"),row.names = FALSE)
    write.csv(visual_data,paste0("./output_validation_test/mds_visual_", year,".csv"),row.names = FALSE)
    write.csv(combination_data,paste0("./output_validation_test/mds_combination_", year,".csv"),row.names = FALSE)
    write.csv(forum_data,paste0("./output_validation_test/mds_forum_", year,".csv"),row.names = FALSE)

}

```

## Correlations

```{r}

load_and_dist <- function(method, approach, year) {
  filepath <- paste0("./output_validation_test/", method, "_", approach, "_", year, ".csv")
  data <- read.csv(filepath, stringsAsFactors = FALSE)
  # data <- data %>% filter(clustering_ids!="2_2" & clustering_ids!="2_4" & clustering_ids!="2_6" & clustering_ids!="3_1" & clustering_ids!="3_4" & clustering_ids!="3_6" & clustering_ids!="14_1" & clustering_ids!="14_2" & clustering_ids!="14_3" & clustering_ids!="14_5" & clustering_ids!="21_1" & clustering_ids!="21_3" & clustering_ids!="21_5" & clustering_ids!="35_2" & clustering_ids!="35_3" & clustering_ids!="37_11" & clustering_ids!="37_2" & clustering_ids!="37_5"  & clustering_ids!="37_6" & clustering_ids!="37_7" & clustering_ids!="37_8"  & clustering_ids!="37_9" & clustering_ids!="55_5" & clustering_ids!="55_9"  & clustering_ids!="68_1" & clustering_ids!="68_2" & clustering_ids!="68_3"  & clustering_ids!="85_1" & clustering_ids!="85_3" & clustering_ids!="85_5"  & clustering_ids!="85_6" & clustering_ids!="85_7" & clustering_ids!="54_1"  & clustering_ids!="54_4" & clustering_ids!="54_6" & clustering_ids!="54_7"  & clustering_ids!="79_1" & clustering_ids!="79_10" & clustering_ids!="79_11" & clustering_ids!="79_12" & clustering_ids!="79_2" & clustering_ids!="79_6" & clustering_ids!="79_8" & clustering_ids!="79_9" & clustering_ids!="84_1" & clustering_ids!="84_2")
  dist_matrix <- as.matrix(dist(data[, c("x", "y")]))
  return(dist_matrix)
}

compare_correlations <- function(start_year=2008, end_year=2017, methods=c("mds")) {
  approaches <- c("structured", "visual", "combination", "forum")
  results <- data.frame(year=integer(), method=character(), 
                        cor_forum_structured=numeric(), 
                        cor_forum_visual=numeric(), 
                        cor_forum_combination=numeric())
  
  for (method in methods) {
    for (year in start_year:end_year) {
      dist_matrices <- list()
      for (approach in approaches) {
        dist_matrices[[approach]] <- load_and_dist(method, approach, year)
      }

      # Extract upper triangles of the matrices as vector
      forum_vec <- as.vector(dist_matrices[["forum"]][upper.tri(dist_matrices[["forum"]])])
      structured_vec <- as.vector(dist_matrices[["structured"]][upper.tri(dist_matrices[["structured"]])])
      visual_vec <- as.vector(dist_matrices[["visual"]][upper.tri(dist_matrices[["visual"]])])
      combination_vec <- as.vector(dist_matrices[["combination"]][upper.tri(dist_matrices[["combination"]])])
      
      # Compute correlations
      cor_forum_structured <- cor(forum_vec, structured_vec)
      cor_forum_visual <- cor(forum_vec, visual_vec)
      cor_forum_combination <- cor(forum_vec, combination_vec)
      
      # Append results to the dataframe
      results <- rbind(results, data.frame(year=year, method=method, 
                            cor_forum_structured=cor_forum_structured, 
                            cor_forum_visual=cor_forum_visual, 
                            cor_forum_combination=cor_forum_combination))
    }
  }
  
  return(results)
}

# Execute the function
correlation_results <- compare_correlations()

mean(abs(correlation_results$cor_forum_structured[correlation_results$method=="mds"]))
mean(abs(correlation_results$cor_forum_visual[correlation_results$method=="mds"]))
mean(abs(correlation_results$cor_forum_combination[correlation_results$method=="mds"]))

mean((correlation_results$cor_forum_structured[correlation_results$method=="mds"]))
mean((correlation_results$cor_forum_visual[correlation_results$method=="mds"]))
mean((correlation_results$cor_forum_combination[correlation_results$method=="mds"]))

```

## RandomForest

```{r}

# results_df <- data.frame(random_seed = integer(), year = integer(), str = numeric(), viz = numeric(), comb = numeric(), str_viz = numeric())
# 
# seeds <- 1:10
# 
# for(seed in seeds){
#   set.seed(seed)
# for(year in 2008:2017){
#   structured_data <- read.csv(paste0("./output_validation_test/mds_structured_",year,".csv"),stringsAsFactors = FALSE)
#   visual_data <- read.csv(paste0("./output_validation_test/mds_visual_",year,".csv"),stringsAsFactors = FALSE) 
#   combination_data <- read.csv(paste0("./output_validation_test/mds_combination_",year,".csv"),stringsAsFactors = FALSE)
#   forum_data <- read.csv(paste0("./output_validation_test/mds_forum_",year,".csv"),stringsAsFactors = FALSE)
# 
#   structured_data <- as.vector(as.matrix(dist(structured_data[, c("x", "y")]))[upper.tri(as.matrix(dist(structured_data[, c("x", "y")])))])
#   visual_data <- as.vector(as.matrix(dist(visual_data[, c("x", "y")]))[upper.tri(as.matrix(dist(visual_data[, c("x", "y")])))])
#   combination_data <- as.vector(as.matrix(dist(combination_data[, c("x", "y")]))[upper.tri(as.matrix(dist(combination_data[, c("x", "y")])))])
#   forum_data <- as.vector(as.matrix(dist(forum_data[, c("x", "y")]))[upper.tri(as.matrix(dist(forum_data[, c("x", "y")])))])
#   
#   df <- data.frame(structured_data = structured_data, visual_data = visual_data, combination_data = combination_data, forum_data = forum_data)
#   
#   trainIndex <- createDataPartition(df$forum_data, p = .8, list = FALSE, times = 1)
# 
#   trainData <- df[ trainIndex,]
#   testData  <- df[-trainIndex,]
#   
#   model_structured <- randomForest(forum_data ~ structured_data, data=trainData)
#   model_visual <- randomForest(forum_data ~ visual_data, data=trainData)
#   model_combination <- randomForest(forum_data ~ combination_data, data=trainData)
#   model_structured_visual <- randomForest(forum_data ~ structured_data + visual_data, data=trainData)
#   
#   pred_structured <- predict(model_structured, testData)
#   pred_visual <- predict(model_visual, testData)
#   pred_combination <- predict(model_combination, testData)
#   pred_structured_visual <- predict(model_structured_visual, testData)
#   
#   mae_structured <- mean(abs(pred_structured - testData$forum_data))
#   mae_visual <- mean(abs(pred_visual - testData$forum_data))
#   mae_combination <- mean(abs(pred_combination - testData$forum_data))
#   mae_structured_visual <- mean(abs(pred_structured_visual - testData$forum_data))
# 
#   results_df <- rbind(results_df, data.frame(random_seed = seed, year = year, str = mae_structured, viz = mae_visual, comb = mae_combination, str_viz = mae_structured_visual))
# 
# 
# }}
# 
# results_df$method <- "mds"
# 
# mds_results_df <- results_df
# 
# mean(mds_results_df$str)
# mean(mds_results_df$viz)
# mean(mds_results_df$comb)
# mean(mds_results_df$str_viz)
# 
# rf_regression_results_df <- mds_results_df








all_structured_data <- list()
all_visual_data <- list()
all_combination_data <- list()
all_forum_data <- list()
structured_dist_list <- list()
visual_dist_list <- list()
combination_dist_list <- list()
forum_dist_list <- list()
for (year in 2008:2017) {
  # Read CSVs
  structured_df <- read.csv(paste0("./output_validation_test/mds_structured_", year, ".csv"), stringsAsFactors = FALSE)
  visual_df <- read.csv(paste0("./output_validation_test/mds_visual_", year, ".csv"), stringsAsFactors = FALSE)
  combo_df <- read.csv(paste0("./output_validation_test/mds_combination_", year, ".csv"), stringsAsFactors = FALSE)
  forum_df <- read.csv(paste0("./output_validation_test/mds_forum_", year, ".csv"), stringsAsFactors = FALSE)
  
  # Store the data in memory
  all_structured_data[[as.character(year)]] <- structured_df
  all_visual_data[[as.character(year)]] <- visual_df
  all_combination_data[[as.character(year)]] <- combo_df
  all_forum_data[[as.character(year)]] <- forum_df
  
  # Precompute distances
  structured_dist_list[[as.character(year)]] <- as.vector(as.matrix(dist(structured_df[, c("x", "y")])))[upper.tri(dist(structured_df[, c("x", "y")]))]
  visual_dist_list[[as.character(year)]] <- as.vector(as.matrix(dist(visual_df[, c("x", "y")])))[upper.tri(dist(visual_df[, c("x", "y")]))]
  combination_dist_list[[as.character(year)]] <- as.vector(as.matrix(dist(combo_df[, c("x", "y")])))[upper.tri(dist(combo_df[, c("x", "y")]))]
  forum_dist_list[[as.character(year)]] <- as.vector(as.matrix(dist(forum_df[, c("x", "y")])))[upper.tri(dist(forum_df[, c("x", "y")]))]
}

num_cores <- parallel::detectCores() - 1
cl <- makeCluster(num_cores)
registerDoParallel(cl)

results_df <- foreach(seed = 1:10, .combine = rbind) %:%
  foreach(year = 2008:2017, .combine = rbind) %dopar% {
    library(caret)  # Required for createDataPartition
    library(ranger) # Required inside parallel processes
    library(dplyr)  # Required inside parallel processes
    
    set.seed(seed)
    
    # Retrieve precomputed distances
    structured_data <- structured_dist_list[[as.character(year)]]
    visual_data <- visual_dist_list[[as.character(year)]]
    combination_data <- combination_dist_list[[as.character(year)]]
    forum_data <- forum_dist_list[[as.character(year)]]
    
    # Create a data frame for modeling
    df <- data.frame(
      structured_data = structured_data,
      visual_data = visual_data,
      combination_data = combination_data,
      forum_data = forum_data
    )
    
    # Train/test split
    trainIndex <- createDataPartition(df$forum_data, p = 0.8, list = FALSE, times = 1)
    trainData <- df[trainIndex,]
    testData <- df[-trainIndex,]
    
    # Fit Random Forest models using `ranger`
    model_structured <- ranger(forum_data ~ structured_data, data = trainData, num.trees = 100)
    model_visual <- ranger(forum_data ~ visual_data, data = trainData, num.trees = 100)
    model_combination <- ranger(forum_data ~ combination_data, data = trainData, num.trees = 100)
    model_structured_visual <- ranger(forum_data ~ structured_data + visual_data, data = trainData, num.trees = 100)
    
    # Predictions
    pred_structured <- predict(model_structured, data = testData)$predictions
    pred_visual <- predict(model_visual, data = testData)$predictions
    pred_combination <- predict(model_combination, data = testData)$predictions
    pred_structured_visual <- predict(model_structured_visual, data = testData)$predictions
    
    # Compute MAE
    mae_structured <- mean(abs(pred_structured - testData$forum_data))
    mae_visual <- mean(abs(pred_visual - testData$forum_data))
    mae_combination <- mean(abs(pred_combination - testData$forum_data))
    mae_structured_visual <- mean(abs(pred_structured_visual - testData$forum_data))
    
    # Return the results as a one-row data frame
    data.frame(
      random_seed = seed,
      year = year,
      str = mae_structured,
      viz = mae_visual,
      comb = mae_combination,
      str_viz = mae_structured_visual
    )
  }

stopCluster(cl)

# Combine results into a single data frame
results_df$method <- "mds"

# Step 3: Summarize Results
cat("Mean MAE (Structured):", mean(results_df$str), "\n")
cat("Mean MAE (Visual):", mean(results_df$viz), "\n")
cat("Mean MAE (Combination):", mean(results_df$comb), "\n")
cat("Mean MAE (Structured + Visual):", mean(results_df$str_viz), "\n")

```

## XGBoost

```{r}

results_df <- data.frame(random_seed = integer(), year = integer(), str = numeric(), viz = numeric(), comb = numeric(), str_viz = numeric())

seeds <- 1:10

for(seed in seeds){
  set.seed(seed)
for(year in 2008:2017){
  structured_data <- read.csv(paste0("./output_validation_test/mds_structured_",year,".csv"),stringsAsFactors = FALSE)
  visual_data <- read.csv(paste0("./output_validation_test/mds_visual_",year,".csv"),stringsAsFactors = FALSE) 
  combination_data <- read.csv(paste0("./output_validation_test/mds_combination_",year,".csv"),stringsAsFactors = FALSE)
  forum_data <- read.csv(paste0("./output_validation_test/mds_forum_",year,".csv"),stringsAsFactors = FALSE)

  structured_data <- as.vector(as.matrix(dist(structured_data[, c("x", "y")]))[upper.tri(as.matrix(dist(structured_data[, c("x", "y")])))])
  visual_data <- as.vector(as.matrix(dist(visual_data[, c("x", "y")]))[upper.tri(as.matrix(dist(visual_data[, c("x", "y")])))])
  combination_data <- as.vector(as.matrix(dist(combination_data[, c("x", "y")]))[upper.tri(as.matrix(dist(combination_data[, c("x", "y")])))])
  forum_data <- as.vector(as.matrix(dist(forum_data[, c("x", "y")]))[upper.tri(as.matrix(dist(forum_data[, c("x", "y")])))])
  
  df <- data.frame(structured_data = structured_data, visual_data = visual_data, combination_data = combination_data, forum_data = forum_data)
  
  trainIndex <- createDataPartition(df$forum_data, p = .8, list = FALSE, times = 1)

  trainData <- df[ trainIndex,]
  testData  <- df[-trainIndex,]
  
  dtrain <- xgb.DMatrix(data = as.matrix(trainData[, -4]), label = trainData$forum_data)
  dtest <- xgb.DMatrix(data = as.matrix(testData[, -4]), label = testData$forum_data)
  
  params <- list(booster = "gbtree", objective = "reg:squarederror", eta = 0.3, max_depth = 6)
  
  model_structured <- xgb.train(params, dtrain, nrounds = 100, watchlist = list(train = dtrain, test = dtest), print_every_n = 10)
  model_visual <- xgb.train(params, xgb.DMatrix(data = as.matrix(trainData[, c("visual_data")]), label = trainData$forum_data), nrounds = 100)
  model_combination <- xgb.train(params, xgb.DMatrix(data = as.matrix(trainData[, c("combination_data")]), label = trainData$forum_data), nrounds = 100)
  model_structured_visual <- xgb.train(params, xgb.DMatrix(data = as.matrix(trainData[, c("structured_data", "visual_data")]), label = trainData$forum_data), nrounds = 100)
  
  pred_structured <- predict(model_structured, dtest)
  pred_visual <- predict(model_visual, as.matrix(testData[, c("visual_data")]))
  pred_combination <- predict(model_combination, as.matrix(testData[, c("combination_data")]))
  pred_structured_visual <- predict(model_structured_visual, as.matrix(testData[, c("structured_data", "visual_data")]))
  
  mae_structured <- mean(abs(pred_structured - testData$forum_data))
  mae_visual <- mean(abs(pred_visual - testData$forum_data))
  mae_combination <- mean(abs(pred_combination - testData$forum_data))
  mae_structured_visual <- mean(abs(pred_structured_visual - testData$forum_data))

results_df <- rbind(results_df, data.frame(random_seed = seed, year = year, str = mae_structured, viz = mae_visual, comb = mae_combination, str_viz = mae_structured_visual))

}}

results_df$method <- "mds"

mds_results_df <- results_df

mean(mds_results_df$str)
mean(mds_results_df$viz)
mean(mds_results_df$comb)
mean(mds_results_df$str_viz)

xg_regression_results_df <- mds_results_df

```

## Combining RF and XGB

```{r}

oos_results_df <- rbind(rf_regression_results_df,xg_regression_results_df)

```

